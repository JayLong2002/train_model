{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import Accuracy\n",
    "import torch.nn.init as init\n",
    "\n",
    "# 更改工作目录\n",
    "os.chdir('/home/ubuntu/zlb/MEIJU/train_model')\n",
    "\n",
    "import config as cf\n",
    "\n",
    "# 定义的模型\n",
    "from transformer_model import MultimodalDataset,SingleModalEmotionRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要指定具体需要单独训练transformer的模态\n",
    "# text , audio , video \n",
    "\n",
    "cf.TRAIN_DIM = 'audio'\n",
    "\n",
    "print(f\"当前单独训练的模态是{cf.TRAIN_DIM}\")\n",
    "\n",
    "single_modality_mapping = {'text':0,'audio':1,'video':2}\n",
    "\n",
    "# 获取单个特征的维数\n",
    "DIM = cf.get_feature_dim()[single_modality_mapping[cf.TRAIN_DIM]]\n",
    "\n",
    "print(f\"模态{cf.TRAIN_DIM}的向量维数为{DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个transformer需要搜寻的最优超参数组合\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.0001],\n",
    "    'batch_size': [32,16],\n",
    "    'head': [16], \n",
    "    'layers':[2],\n",
    "    'epochs': [5], \n",
    "    #'dropout_rate': [0.1,0.3], # 0.1 - 0.3 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建和训练集\n",
    "train_dataset = MultimodalDataset(mode='train')\n",
    "val_dataset = MultimodalDataset(mode='valid')\n",
    "\n",
    "#定义模型训练的辅助函数\n",
    "def harmonic_mean(M_emotion, M_intent):\n",
    "    \"\"\"\n",
    "    计算比赛用的指标JRPM\n",
    "    \"\"\"\n",
    "    return 2 * M_emotion * M_intent / (M_emotion + M_intent + 1e-6)\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    用于将模型kaiming初始化的辅助函数\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 下面的代码是在验证集上面搜索最优的超参数\n",
    "\"\"\" \n",
    "\n",
    "best_params = None\n",
    "best_accuracy = 0  # Track highest accuracy instead of loss\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    # 加载数据集\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = SingleModalEmotionRecognition(\n",
    "        feature_dim = DIM,\n",
    "        intent_classes=8, emotion_classes=7, \n",
    "        head=params['head'],  # 头数\n",
    "        layers=params['layers'], # 维数\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    #kaiming 初始化\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 统计每一轮的训练集loss和验证集loss，画图\n",
    "    train_loss_values = []\n",
    "    val_loss_values = []\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# ---------------------------------------------模型训练---------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # 训练模型\n",
    "    for epoch in range(params['epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for text, audio,video,intent_label, emotion_label in train_loader:\n",
    "            \n",
    "            feature = (text,audio,video)[single_modality_mapping[cf.TRAIN_DIM]]\n",
    "            feature = feature.to(DEVICE)\n",
    "            \n",
    "            assert DIM == feature.shape[1],\"feature dim don't match\"\n",
    "\n",
    "            intent_label, emotion_label = intent_label.to(DEVICE), emotion_label.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            intent_output, emotion_output = model(feature)\n",
    "            \n",
    "            loss_intent = criterion(intent_output, intent_label)\n",
    "            loss_emotion = criterion(emotion_output, emotion_label)\n",
    "\n",
    "            loss = loss_intent + loss_emotion #合并loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_loss_values.append(avg_loss)\n",
    "\n",
    "        # 模型验证阶段\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for text,audio,video,intent_label,emotion_label in val_loader:\n",
    "\n",
    "                feature = (text,audio,video)[single_modality_mapping[cf.TRAIN_DIM]]\n",
    "                feature = feature.to(DEVICE)\n",
    "                assert DIM == feature.shape[1],\"feature dim don't match\"\n",
    "\n",
    "                intent_label, emotion_label = intent_label.to(DEVICE), emotion_label.to(DEVICE)\n",
    "                intent_output, emotion_output = model(feature)\n",
    "                loss_intent = criterion(intent_output, intent_label)\n",
    "                loss_emotion = criterion(emotion_output, emotion_label)\n",
    "                loss = loss_intent + loss_emotion\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_loss_values.append(avg_val_loss)  # 保存验证集loss    \n",
    "\n",
    "    plt.plot(range(1, params['epochs'] + 1), train_loss_values, marker='o', label='Training Loss', color='blue')\n",
    "    plt.plot(range(1, params['epochs'] + 1), val_loss_values, marker='x', label='Validation Loss', color='red')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# ---------------------------------------------模型评测---------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # 用于计算正确率的辅助变量\n",
    "    intent_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=8).to(DEVICE)\n",
    "    emotion_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=7).to(DEVICE)\n",
    "    majority_intent_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=8).to(DEVICE)\n",
    "    majority_emotion_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=7).to(DEVICE)\n",
    "    less_intent_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=8).to(DEVICE)\n",
    "    less_emotion_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=7).to(DEVICE)\n",
    "    total = 0\n",
    "\n",
    "    # 用于计算JRPM指标的辅助变量\n",
    "    all_intent_preds = []\n",
    "    all_intent_labels = []\n",
    "    all_emotion_preds = []\n",
    "    all_emotion_labels = []\n",
    "\n",
    "    # 这个循环会对 params 所生成的模型进行保存\n",
    "    with torch.no_grad():\n",
    "        for text, audio,video,intent_label,emotion_label in val_loader:\n",
    "\n",
    "            feature = (text,audio,video)[single_modality_mapping[cf.TRAIN_DIM]]\n",
    "            feature = feature.to(DEVICE)\n",
    "            assert DIM == feature.shape[1],\"feature dim don't match\"\n",
    "            intent_label, emotion_label = intent_label.to(DEVICE), emotion_label.to(DEVICE)\n",
    "            \n",
    "            intent_output, emotion_output = model(feature)\n",
    "\n",
    "            _, predicted_intent = torch.max(intent_output, 1)\n",
    "            _, predicted_emotion = torch.max(emotion_output, 1)\n",
    "\n",
    "            all_intent_preds.extend(predicted_intent.cpu().numpy())\n",
    "            all_intent_labels.extend(intent_label.cpu().numpy())\n",
    "            all_emotion_preds.extend(predicted_emotion.cpu().numpy())\n",
    "            all_emotion_labels.extend(emotion_label.cpu().numpy())\n",
    "            \n",
    "            # 筛选出 emotion 和 intent 多数标签的 mask\n",
    "            mask1 = torch.isin(intent_label, torch.tensor([4, 0, 3], device=DEVICE))\n",
    "            mask2 = torch.isin(intent_label, torch.tensor([4, 5], device=DEVICE))\n",
    "\n",
    "            # 更新总体准确率\n",
    "            intent_accuracy_metric.update(predicted_intent, intent_label)\n",
    "            emotion_accuracy_metric.update(predicted_emotion, emotion_label)\n",
    "            \n",
    "            # 更新多数类别的准确率\n",
    "            if mask2.sum() > 0:\n",
    "                majority_intent_accuracy_metric.update(predicted_intent[mask2], intent_label[mask2])\n",
    "            if mask1.sum() > 0:\n",
    "                majority_emotion_accuracy_metric.update(predicted_emotion[mask1], emotion_label[mask1])\n",
    "            \n",
    "            # 更新少数类别的准确率\n",
    "            if (total - mask2.sum().item()) > 0 and (predicted_intent[~mask2].shape[0] > 0):\n",
    "                less_intent_accuracy_metric.update(predicted_intent[~mask2], intent_label[~mask2])\n",
    "            if (total - mask1.sum().item()) > 0 and (predicted_emotion[~mask1].shape[0] > 0):\n",
    "                less_emotion_accuracy_metric.update(predicted_emotion[~mask1], emotion_label[~mask1])\n",
    "                \n",
    "            total += intent_label.size(0)\n",
    "\n",
    "    #计算准确率\n",
    "    intent_accuracy = intent_accuracy_metric.compute()\n",
    "    emotion_accuracy = emotion_accuracy_metric.compute()\n",
    "    majority_intent_accuracy = majority_intent_accuracy_metric.compute()\n",
    "    majority_emotion_accuracy = majority_emotion_accuracy_metric.compute()\n",
    "    less_intent_accuracy = less_intent_accuracy_metric.compute()\n",
    "    less_emotion_accuracy = less_emotion_accuracy_metric.compute()\n",
    "    avg_accuracy = (intent_accuracy + emotion_accuracy) / 2\n",
    "\n",
    "    #计算JRPM(比赛用的指标)\n",
    "    intent_f1 = f1_score(all_intent_labels, all_intent_preds, average='micro')\n",
    "    emotion_f1 = f1_score(all_emotion_labels, all_emotion_preds, average='micro')\n",
    "    joint_metrics = harmonic_mean(emotion_f1, intent_f1)\n",
    "\n",
    "    # 保存最佳模型参数\n",
    "    if joint_metrics > best_accuracy:\n",
    "        best_accuracy = joint_metrics\n",
    "        best_params = params\n",
    "\n",
    "    print(f\"Best Transformer Params in {cf.TRAIN_DIM} dim\")\n",
    "    print(f\"Params: {params}\")\n",
    "    print(f\"Intent Accuracy: {intent_accuracy.item():.4f}\")\n",
    "    print(f\"Majority Intent Accuracy: {majority_intent_accuracy.item():.4f}\")\n",
    "    print(f\"Less Frequent Intent Accuracy: {less_intent_accuracy.item():.4f}\")\n",
    "    print(f\"Emotion Accuracy: {emotion_accuracy.item():.4f}\")\n",
    "    print(f\"Majority Emotion Accuracy: {majority_emotion_accuracy.item():.4f}\")\n",
    "    print(f\"Less Frequent Emotion Accuracy: {less_emotion_accuracy.item():.4f}\")\n",
    "    print(f\"Average Accuracy: {avg_accuracy.item():.4f}\")\n",
    "    print(f\"f1-score Intent{intent_f1:.4f}\")\n",
    "    print(f\"f1-score Emotion{emotion_f1:4f}\")\n",
    "    print(f\"JRBM: {joint_metrics:4f}\")\n",
    "    \n",
    "print(f\"Best Parameters: {best_params}, Best JRBM: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 保存之前搜寻到的最优超参数对应的模型, 模型保存路径在 config.py 定义\n",
    "2. 在测试集上测试，生成测试结果\n",
    "\"\"\"\n",
    "\n",
    "date = \"10_30\"\n",
    "# 需要修改，指定模型的名字\n",
    "model_name = f\"{cf.TRAIN_DIM}_transformer_{date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "不用看，直接折叠\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# ---------------------------------------------下面不用修改--------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 模型会保存在这个地址，其中EST_MODEL_SAVE_DIR是在config.py中定义的\n",
    "model_save_path = f\"{cf.BEST_MODEL_SAVE_DIR}{model_name}\"\n",
    "# 也可以直接指定比如 model_save_path =  \"/home/ubuntu/zlb/MEIJU/model/save/10_31_model\"\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# ---------------------------------------------下面是重新训练最好参数的模型并且保存，代码会很长,不用看--------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "params = best_params\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载数据集\n",
    "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "# 初始化模型\n",
    "model = SingleModalEmotionRecognition(\n",
    "    feature_dim = DIM,\n",
    "    intent_classes=8, emotion_classes=7, \n",
    "    head=params['head'],  # 头数\n",
    "    layers=params['layers'], # 维数\n",
    ").to(DEVICE)\n",
    "\n",
    "#kaiming 初始化\n",
    "model.apply(init_weights)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 统计每一轮的训练集loss和验证集loss，画图\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# ---------------------------------------------模型训练---------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(params['epochs']):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for text, audio,video,intent_label, emotion_label in train_loader:\n",
    "        \n",
    "        feature = (text,audio,video)[single_modality_mapping[cf.TRAIN_DIM]]\n",
    "        feature = feature.to(DEVICE)\n",
    "        \n",
    "        assert DIM == feature.shape[1],\"feature dim don't match\"\n",
    "\n",
    "        intent_label, emotion_label = intent_label.to(DEVICE), emotion_label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        intent_output, emotion_output = model(feature)\n",
    "        \n",
    "        loss_intent = criterion(intent_output, intent_label)\n",
    "        loss_emotion = criterion(emotion_output, emotion_label)\n",
    "\n",
    "        loss = loss_intent + loss_emotion #合并loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_loss_values.append(avg_loss)\n",
    "\n",
    "    # 模型验证阶段\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for text,audio,video,intent_label,emotion_label in val_loader:\n",
    "\n",
    "            feature = (text,audio,video)[single_modality_mapping[cf.TRAIN_DIM]]\n",
    "            feature = feature.to(DEVICE)\n",
    "            assert DIM == feature.shape[1],\"feature dim don't match\"\n",
    "\n",
    "            intent_label, emotion_label = intent_label.to(DEVICE), emotion_label.to(DEVICE)\n",
    "            intent_output, emotion_output = model(feature)\n",
    "            loss_intent = criterion(intent_output, intent_label)\n",
    "            loss_emotion = criterion(emotion_output, emotion_label)\n",
    "            loss = loss_intent + loss_emotion\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_loss_values.append(avg_val_loss)  # 保存验证集loss    \n",
    "\n",
    "plt.plot(range(1, params['epochs'] + 1), train_loss_values, marker='o', label='Training Loss', color='blue')\n",
    "plt.plot(range(1, params['epochs'] + 1), val_loss_values, marker='x', label='Validation Loss', color='red')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# ---------------------------------------------模型评测---------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 用于计算正确率的辅助变量\n",
    "intent_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=8).to(DEVICE)\n",
    "emotion_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=7).to(DEVICE)\n",
    "majority_intent_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=8).to(DEVICE)\n",
    "majority_emotion_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=7).to(DEVICE)\n",
    "less_intent_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=8).to(DEVICE)\n",
    "less_emotion_accuracy_metric = Accuracy(task=\"multiclass\",num_classes=7).to(DEVICE)\n",
    "total = 0\n",
    "\n",
    "# 用于计算JRPM指标的辅助变量\n",
    "all_intent_preds = []\n",
    "all_intent_labels = []\n",
    "all_emotion_preds = []\n",
    "all_emotion_labels = []\n",
    "\n",
    "# 这个循环会对 params 所生成的模型进行保存\n",
    "with torch.no_grad():\n",
    "    for text, audio,video,intent_label,emotion_label in val_loader:\n",
    "\n",
    "        feature = (text,audio,video)[single_modality_mapping[cf.TRAIN_DIM]]\n",
    "        feature = feature.to(DEVICE)\n",
    "        assert DIM == feature.shape[1],\"feature dim don't match\"\n",
    "        intent_label, emotion_label = intent_label.to(DEVICE), emotion_label.to(DEVICE)\n",
    "        \n",
    "        intent_output, emotion_output = model(feature)\n",
    "\n",
    "        _, predicted_intent = torch.max(intent_output, 1)\n",
    "        _, predicted_emotion = torch.max(emotion_output, 1)\n",
    "\n",
    "        all_intent_preds.extend(predicted_intent.cpu().numpy())\n",
    "        all_intent_labels.extend(intent_label.cpu().numpy())\n",
    "        all_emotion_preds.extend(predicted_emotion.cpu().numpy())\n",
    "        all_emotion_labels.extend(emotion_label.cpu().numpy())\n",
    "        \n",
    "        # 筛选出 emotion 和 intent 多数标签的 mask\n",
    "        mask1 = torch.isin(intent_label, torch.tensor([4, 0, 3], device=DEVICE))\n",
    "        mask2 = torch.isin(intent_label, torch.tensor([4, 5], device=DEVICE))\n",
    "\n",
    "        # 更新总体准确率\n",
    "        intent_accuracy_metric.update(predicted_intent, intent_label)\n",
    "        emotion_accuracy_metric.update(predicted_emotion, emotion_label)\n",
    "        \n",
    "        # 更新多数类别的准确率\n",
    "        if mask2.sum() > 0:\n",
    "            majority_intent_accuracy_metric.update(predicted_intent[mask2], intent_label[mask2])\n",
    "        if mask1.sum() > 0:\n",
    "            majority_emotion_accuracy_metric.update(predicted_emotion[mask1], emotion_label[mask1])\n",
    "        \n",
    "        # 更新少数类别的准确率\n",
    "        if (total - mask2.sum().item()) > 0 and (predicted_intent[~mask2].shape[0] > 0):\n",
    "            less_intent_accuracy_metric.update(predicted_intent[~mask2], intent_label[~mask2])\n",
    "        if (total - mask1.sum().item()) > 0 and (predicted_emotion[~mask1].shape[0] > 0):\n",
    "            less_emotion_accuracy_metric.update(predicted_emotion[~mask1], emotion_label[~mask1])\n",
    "            \n",
    "        total += intent_label.size(0)\n",
    "\n",
    "#计算准确率\n",
    "intent_accuracy = intent_accuracy_metric.compute()\n",
    "emotion_accuracy = emotion_accuracy_metric.compute()\n",
    "majority_intent_accuracy = majority_intent_accuracy_metric.compute()\n",
    "majority_emotion_accuracy = majority_emotion_accuracy_metric.compute()\n",
    "less_intent_accuracy = less_intent_accuracy_metric.compute()\n",
    "less_emotion_accuracy = less_emotion_accuracy_metric.compute()\n",
    "avg_accuracy = (intent_accuracy + emotion_accuracy) / 2\n",
    "\n",
    "#计算JRPM(比赛用的指标)\n",
    "intent_f1 = f1_score(all_intent_labels, all_intent_preds, average='micro')\n",
    "emotion_f1 = f1_score(all_emotion_labels, all_emotion_preds, average='micro')\n",
    "joint_metrics = harmonic_mean(emotion_f1, intent_f1)\n",
    "\n",
    "# 保存最佳模型参数\n",
    "if joint_metrics > best_accuracy:\n",
    "    best_accuracy = joint_metrics\n",
    "    best_params = params\n",
    "\n",
    "print(f\"Best Transformer Params in {cf.TRAIN_DIM} dim\")\n",
    "print(f\"Params: {params}\")\n",
    "print(f\"Intent Accuracy: {intent_accuracy.item():.4f}\")\n",
    "print(f\"Majority Intent Accuracy: {majority_intent_accuracy.item():.4f}\")\n",
    "print(f\"Less Frequent Intent Accuracy: {less_intent_accuracy.item():.4f}\")\n",
    "print(f\"Emotion Accuracy: {emotion_accuracy.item():.4f}\")\n",
    "print(f\"Majority Emotion Accuracy: {majority_emotion_accuracy.item():.4f}\")\n",
    "print(f\"Less Frequent Emotion Accuracy: {less_emotion_accuracy.item():.4f}\")\n",
    "print(f\"Average Accuracy: {avg_accuracy.item():.4f}\")\n",
    "print(f\"f1-score Intent{intent_f1:.4f}\")\n",
    "print(f\"f1-score Emotion{emotion_f1:4f}\")\n",
    "print(f\"JRBM: {joint_metrics:4f}\")\n",
    "    \n",
    "print(f'{cf.TRAIN_DIM}模态的tansformer模型单独保存到：{model_save_path}.pth')\n",
    "print(f'该模型的设置为头数{params['head']}层数{params['layers']}')\n",
    "torch.save(model.transformer.state_dict(), f'{model_save_path}.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MEIJU",
   "language": "python",
   "name": "meiju"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
